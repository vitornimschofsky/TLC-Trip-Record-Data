{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442bf564-b39b-4045-98be-511f6afb556b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce, when, year, month, date_format, regexp_extract\n",
    "from pyspark.sql.types import TimestampType, DoubleType, IntegerType, StringType\n",
    "\n",
    "\n",
    "\n",
    "aws_access_key_id = \"suacredencial\"\n",
    "aws_secret_access_key = \"suacredencial\"\n",
    "s3_bucket_name = \"ifood-case-data-lake-vitor\"\n",
    "\n",
    "bronze_path = f\"s3a://{s3_bucket_name}/bronze_layer/\"\n",
    "silver_path = f\"s3a://{s3_bucket_name}/silver_layer/\"\n",
    "\n",
    "read_options = {\"fs.s3a.access.key\": aws_access_key_id, \"fs.s3a.secret.key\": aws_secret_access_key}\n",
    "write_options = {\"fs.s3a.access.key\": aws_access_key_id, \"fs.s3a.secret.key\": aws_secret_access_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22db9533-9d24-43a8-950c-39a6aa952ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de yellow padronizados e salvos na camada Silver.\n"
     ]
    }
   ],
   "source": [
    "# Processamento e unificação dos dados de táxis Yellow\n",
    "df_yellow_jan = (spark.read.options(**read_options).parquet(f\"{bronze_path}yellow/year=2023/month=1\")\n",
    "    .withColumn(\"year\", lit(2023))\n",
    "    .withColumn(\"month\", lit(1))\n",
    ")\n",
    "df_yellow_feb = (spark.read.options(**read_options).parquet(f\"{bronze_path}yellow/year=2023/month=2\")\n",
    "    .withColumn(\"year\", lit(2023))\n",
    "    .withColumn(\"month\", lit(2))\n",
    ")\n",
    "df_yellow_mar = (spark.read.options(**read_options).parquet(f\"{bronze_path}yellow/year=2023/month=3\")\n",
    "    .withColumn(\"year\", lit(2023))\n",
    "    .withColumn(\"month\", lit(3))\n",
    ")\n",
    "df_yellow_apr = (spark.read.options(**read_options).parquet(f\"{bronze_path}yellow/year=2023/month=4\")\n",
    "    .withColumn(\"year\", lit(2023))\n",
    "    .withColumn(\"month\", lit(4))\n",
    ")\n",
    "df_yellow_may = (spark.read.options(**read_options).parquet(f\"{bronze_path}yellow/year=2023/month=5\")\n",
    "    .withColumn(\"year\", lit(2023))\n",
    "    .withColumn(\"month\", lit(5))\n",
    ")\n",
    "\n",
    "df_yellow_unified = (df_yellow_jan.unionByName(df_yellow_feb, allowMissingColumns=True)\n",
    "                             .unionByName(df_yellow_mar, allowMissingColumns=True)\n",
    "                             .unionByName(df_yellow_apr, allowMissingColumns=True)\n",
    "                             .unionByName(df_yellow_may, allowMissingColumns=True))\n",
    "\n",
    "df_yellow_silver = (df_yellow_unified\n",
    "    .withColumn(\"taxi_type\", lit(\"yellow\"))\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "    .withColumnRenamed(\"VendorID\", \"vendor_id\")\n",
    "    .select(\n",
    "        col(\"taxi_type\").cast(StringType()),\n",
    "        col(\"vendor_id\").cast(StringType()),\n",
    "        coalesce(col(\"passenger_count\").cast(DoubleType()), lit(0.0)).cast(IntegerType()).alias(\"passenger_count\"),\n",
    "        col(\"total_amount\").cast(DoubleType()),\n",
    "        col(\"pickup_datetime\").cast(TimestampType()),\n",
    "        col(\"dropoff_datetime\").cast(TimestampType()),\n",
    "        col(\"year\").cast(IntegerType()),\n",
    "        col(\"month\").cast(IntegerType())\n",
    "    )\n",
    "    .withColumn(\"is_valid_total_amount\", when(col(\"total_amount\") > 0, True).otherwise(False))\n",
    "    .withColumn(\"is_valid_passenger_count\", when(col(\"passenger_count\") > 0, True).otherwise(False))\n",
    "    .withColumn(\"is_valid_trip_time\", when(col(\"dropoff_datetime\") > col(\"pickup_datetime\"), True).otherwise(False))\n",
    ")\n",
    "# Escrita em tabelas separadas na camada Silver\n",
    "#df_yellow_silver.write.options(**write_options).option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}yellow/\")\n",
    "(df_yellow_silver.coalesce(1).write.options(**write_options)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"{silver_path}yellow/\")\n",
    ")\n",
    "print(\"Dados de yellow padronizados e salvos na camada Silver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b61290a-90ec-4b3f-984a-2729fa473b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de green padronizados e salvos na camada Silver.\n"
     ]
    }
   ],
   "source": [
    "# Leitura e padronização dos dados de táxis Green\n",
    "df_green_jan = (spark.read.options(**read_options).parquet(f\"{bronze_path}green/year=2023/month=1\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(1))\n",
    ")\n",
    "df_green_feb = (spark.read.options(**read_options).parquet(f\"{bronze_path}green/year=2023/month=2\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(2))\n",
    ")\n",
    "df_green_mar = (spark.read.options(**read_options).parquet(f\"{bronze_path}green/year=2023/month=3\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(3))\n",
    ")\n",
    "df_green_apr = (spark.read.options(**read_options).parquet(f\"{bronze_path}green/year=2023/month=4\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(4))\n",
    ")\n",
    "df_green_may = (spark.read.options(**read_options).parquet(f\"{bronze_path}green/year=2023/month=5\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(5))\n",
    ")\n",
    "\n",
    "df_green_unified = (df_green_jan.unionByName(df_green_feb, allowMissingColumns=True)\n",
    "                             .unionByName(df_green_mar, allowMissingColumns=True)\n",
    "                             .unionByName(df_green_apr, allowMissingColumns=True)\n",
    "                             .unionByName(df_green_may, allowMissingColumns=True))\n",
    "\n",
    "df_green_silver = (df_green_unified\n",
    "    .withColumn(\"taxi_type\", lit(\"green\"))\n",
    "    .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "    .withColumnRenamed(\"VendorID\", \"vendor_id\")\n",
    "    .select(\n",
    "        col(\"taxi_type\").cast(StringType()),\n",
    "        col(\"vendor_id\").cast(StringType()),\n",
    "        coalesce(col(\"passenger_count\").cast(DoubleType()), lit(0.0)).cast(IntegerType()).alias(\"passenger_count\"),\n",
    "        col(\"total_amount\").cast(DoubleType()),\n",
    "        col(\"pickup_datetime\").cast(TimestampType()),\n",
    "        col(\"dropoff_datetime\").cast(TimestampType()),\n",
    "        col(\"year\").cast(IntegerType()),\n",
    "        col(\"month\").cast(IntegerType())\n",
    "    )\n",
    "    .withColumn(\"is_valid_total_amount\", when(col(\"total_amount\") > 0, True).otherwise(False))\n",
    "    .withColumn(\"is_valid_passenger_count\", when(col(\"passenger_count\") > 0, True).otherwise(False))\n",
    "    .withColumn(\"is_valid_trip_time\", when(col(\"dropoff_datetime\") > col(\"pickup_datetime\"), True).otherwise(False))\n",
    ")\n",
    "#df_green_silver.write.options(**write_options).option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}green/\")\n",
    "(df_green_silver.coalesce(1).write.options(**write_options)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"{silver_path}green/\")\n",
    ")\n",
    "print(\"Dados de green padronizados e salvos na camada Silver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594bffed-536d-4ebb-a939-a1084838015b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de fhv padronizados e salvos na camada Silver.\n"
     ]
    }
   ],
   "source": [
    "# Leitura e padronização dos dados de táxis Green\n",
    "df_fhv_jan = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhv/year=2023/month=1\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(1))\n",
    ")\n",
    "df_fhv_feb = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhv/year=2023/month=2\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(2))\n",
    ")\n",
    "df_fhv_mar = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhv/year=2023/month=3\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(3))\n",
    ")\n",
    "df_fhv_apr = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhv/year=2023/month=4\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(4))\n",
    ")\n",
    "df_fhv_may = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhv/year=2023/month=5\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(5))\n",
    ")\n",
    "df_fhv_unified = (df_fhv_jan.unionByName(df_fhv_feb, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhv_mar, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhv_apr, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhv_may, allowMissingColumns=True))\n",
    "\n",
    "df_fhv_silver = (df_fhv_unified\n",
    "    .withColumn(\"taxi_type\", lit(\"fhv\"))\n",
    "    .withColumn(\"pickup_datetime\", col(\"pickup_datetime\").cast(TimestampType()))\n",
    "    .withColumn(\"dropoff_datetime\", col(\"dropOff_datetime\").cast(TimestampType()))\n",
    "    .withColumnRenamed(\"dispatching_base_num\", \"vendor_id\")\n",
    "    .withColumn(\"passenger_count\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"total_amount\", lit(None).cast(DoubleType()))\n",
    "    .select(\n",
    "        col(\"taxi_type\"),\n",
    "        col(\"vendor_id\"),\n",
    "        col(\"passenger_count\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"pickup_datetime\"),\n",
    "        col(\"dropoff_datetime\"),\n",
    "        col(\"year\").cast(IntegerType()),\n",
    "        col(\"month\").cast(IntegerType())\n",
    "    )\n",
    "    .withColumn(\"is_valid_total_amount\", lit(None).cast(\"boolean\"))\n",
    "    .withColumn(\"is_valid_passenger_count\", lit(None).cast(\"boolean\"))\n",
    "    .withColumn(\"is_valid_trip_time\", when(col(\"dropoff_datetime\") > col(\"pickup_datetime\"), True).otherwise(False))\n",
    ")\n",
    "#df_fhv_silver.write.options(**write_options).option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}fhv/\")\n",
    "(df_fhv_silver.coalesce(1).write.options(**write_options)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"{silver_path}fhv/\")\n",
    ")\n",
    "print(\"Dados de fhv padronizados e salvos na camada Silver.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82f2a52-6945-49ae-9768-1fb4ed125d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de fhvhv padronizados e salvos na camada Silver.\n"
     ]
    }
   ],
   "source": [
    "# Leitura e padronização dos dados de táxis Green\n",
    "df_fhvhv_jan = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhvhv/year=2023/month=1\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(1))\n",
    ")\n",
    "df_fhvhv_feb = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhvhv/year=2023/month=2\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(2))\n",
    ")\n",
    "df_fhvhv_mar = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhvhv/year=2023/month=3\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(3))\n",
    ")\n",
    "df_fhvhv_apr = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhvhv/year=2023/month=4\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(4))\n",
    ")\n",
    "df_fhvhv_may = (spark.read.options(**read_options).parquet(f\"{bronze_path}fhvhv/year=2023/month=5\")\n",
    "    .withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(5))\n",
    ")\n",
    "df_fhvhv_unified = (df_fhvhv_jan.unionByName(df_fhvhv_feb, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhvhv_mar, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhvhv_apr, allowMissingColumns=True)\n",
    "                           .unionByName(df_fhvhv_may, allowMissingColumns=True))\n",
    "\n",
    "df_fhvhv_silver = (df_fhvhv_unified\n",
    "    .withColumn(\"taxi_type\", lit(\"fhvhv\"))\n",
    "    .withColumn(\"pickup_datetime\", col(\"pickup_datetime\").cast(TimestampType()))\n",
    "    .withColumn(\"dropoff_datetime\", col(\"dropOff_datetime\").cast(TimestampType()))\n",
    "    .withColumnRenamed(\"dispatching_base_num\", \"vendor_id\")\n",
    "    .withColumn(\"passenger_count\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"total_amount\", lit(None).cast(DoubleType()))\n",
    "    .select(\n",
    "        col(\"taxi_type\"),\n",
    "        col(\"vendor_id\"),\n",
    "        col(\"passenger_count\"),\n",
    "        col(\"total_amount\"),\n",
    "        col(\"pickup_datetime\"),\n",
    "        col(\"dropoff_datetime\"),\n",
    "        col(\"year\").cast(IntegerType()),\n",
    "        col(\"month\").cast(IntegerType())\n",
    "    )\n",
    "    .withColumn(\"is_valid_total_amount\", lit(None).cast(\"boolean\"))\n",
    "    .withColumn(\"is_valid_passenger_count\", lit(None).cast(\"boolean\"))\n",
    "    .withColumn(\"is_valid_trip_time\", when(col(\"dropoff_datetime\") > col(\"pickup_datetime\"), True).otherwise(False))\n",
    ")\n",
    "#df_fhvhv_silver.write.options(**write_options).option(\"overwriteSchema\", \"true\").format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}fhvhv/\")\n",
    "(df_fhvhv_silver.coalesce(1).write.options(**write_options)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(f\"{silver_path}fhvhv/\")\n",
    ")\n",
    "print(\"Dados de fhvhv padronizados e salvos na camada Silver.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Processamento_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
