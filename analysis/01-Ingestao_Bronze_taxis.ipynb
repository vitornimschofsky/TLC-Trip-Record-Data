{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7874ccec-b80a-43b4-9812-8dee085f9a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definição de credenciais e caminhos\n",
    "from pyspark.sql.functions import year, month, lit, coalesce, col, regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "\n",
    "aws_access_key_id = \"suacredencial\"\n",
    "aws_secret_access_key = \"suacredencial\"\n",
    "s3_bucket_name = \"ifood-case-data-lake-vitor\"\n",
    "\n",
    "# caminho da landing zone com os arquivos Parquet brutos\n",
    "landing_zone_path = f\"s3a://{s3_bucket_name}/landing_zone/\"\n",
    "# caminho para a camada Bronze em formato Delta\n",
    "bronze_path = f\"s3a://{s3_bucket_name}/bronze_layer/\"\n",
    "\n",
    "read_options = {\"fs.s3a.access.key\": aws_access_key_id, \"fs.s3a.secret.key\": aws_secret_access_key}\n",
    "write_options = {\"fs.s3a.access.key\": aws_access_key_id, \"fs.s3a.secret.key\": aws_secret_access_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181a4f0d-bcf8-4b8b-b1d9-a3c0a4cbf47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados brutos de yellow de todos os meses salvos com sucesso na camada Bronze em: s3a://ifood-case-data-lake-vitor/bronze_layer/yellow/\nDados brutos de green de todos os meses salvos com sucesso na camada Bronze em: s3a://ifood-case-data-lake-vitor/bronze_layer/green/\nDados brutos de fhv de todos os meses salvos com sucesso na camada Bronze em: s3a://ifood-case-data-lake-vitor/bronze_layer/fhv/\nDados brutos de fhvhv de todos os meses salvos com sucesso na camada Bronze em: s3a://ifood-case-data-lake-vitor/bronze_layer/fhvhv/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, coalesce, year, month\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Leitura e salvamento na camada Bronze em formato Parquet\n",
    "taxi_types = ['yellow', 'green', 'fhv', 'fhvhv']\n",
    "year_str = '2023'\n",
    "months = ['01', '02', '03', '04', '05']\n",
    "\n",
    "for taxi_type in taxi_types:\n",
    "    df_list = []\n",
    "    \n",
    "    for month_str in months:\n",
    "        filename_prefix = 'fhvhv' if taxi_type == 'fhvhv' else taxi_type\n",
    "        filename = f\"{filename_prefix}_tripdata_{year_str}-{month_str}.parquet\"\n",
    "        source_file_path = f\"{landing_zone_path}{filename}\"\n",
    "        \n",
    "        try:\n",
    "            df_monthly = spark.read.options(**read_options).parquet(source_file_path)\n",
    "            # CONVERSÃO PARA STRING: AQUI ESTÁ A LÓGICA QUE RESOLVE O PROBLEMA DE SCHEMA ..000 O TAL DO SCHEMA DRIFT\n",
    "            for c in df_monthly.columns:\n",
    "                df_monthly = df_monthly.withColumn(c, col(c).cast(StringType()))\n",
    "            df_list.append(df_monthly)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if df_list:\n",
    "        df_raw_unified = df_list[0]\n",
    "        for df_monthly in df_list[1:]:\n",
    "            df_raw_unified = df_raw_unified.unionByName(df_monthly, allowMissingColumns=True)\n",
    "            \n",
    "        #Adiciona as colunas de partição de ano e mês antes de salvar\n",
    "        df_with_partitions = (df_raw_unified\n",
    "            .withColumn(\"standard_pickup_datetime\", coalesce(\n",
    "                *[col(c) for c in [\"tpep_pickup_datetime\", \"lpep_pickup_datetime\", \"pickup_datetime\"] if c in df_raw_unified.columns]\n",
    "            ))\n",
    "            .withColumn(\"year\", year(col(\"standard_pickup_datetime\")).cast(IntegerType()))\n",
    "            .withColumn(\"month\", month(col(\"standard_pickup_datetime\")).cast(IntegerType()))\n",
    "            .filter(col(\"year\") == 2023).filter(col(\"month\").between(1, 5))\n",
    "            .drop(\"standard_pickup_datetime\")\n",
    "        )\n",
    "        \n",
    "        # Reduz o número de arquivos Parquet para 1 por partição\n",
    "        df_with_partitions = df_with_partitions.coalesce(1)\n",
    "        \n",
    "        destination_path = f\"{bronze_path}{taxi_type}/\"\n",
    "        (df_with_partitions.write.options(**write_options)\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"year\", \"month\")\n",
    "            .save(destination_path)\n",
    "        )\n",
    "        print(f\"Dados brutos de {taxi_type} de todos os meses salvos com sucesso na camada Bronze em: {destination_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9976eaf5-7344-4083-99b8-7fc3de991c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Contagem de registros para yellow por partição ---\n+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2023|    1|3066726|\n|2023|    2|2914003|\n|2023|    3|3403660|\n|2023|    4|3288248|\n|2023|    5|3513645|\n+----+-----+-------+\n\nTipo de táxi: yellow, Total de Registros: 16186282\n\n--- Contagem de registros para green por partição ---\n+----+-----+-----+\n|year|month|count|\n+----+-----+-----+\n|2023|    1|68231|\n|2023|    2|64792|\n|2023|    3|72039|\n|2023|    4|65391|\n|2023|    5|69168|\n+----+-----+-----+\n\nTipo de táxi: green, Total de Registros: 339621\n\n--- Contagem de registros para fhv por partição ---\n+----+-----+-------+\n|year|month|  count|\n+----+-----+-------+\n|2023|    1|1114320|\n|2023|    2|1110797|\n|2023|    3|1328242|\n|2023|    4|1246479|\n|2023|    5|1385826|\n+----+-----+-------+\n\nTipo de táxi: fhv, Total de Registros: 6185664\n\n--- Contagem de registros para fhvhv por partição ---\n+----+-----+--------+\n|year|month|   count|\n+----+-----+--------+\n|2023|    1|18479031|\n|2023|    2|17960971|\n|2023|    3|20413539|\n|2023|    4|19144903|\n|2023|    5|19847676|\n+----+-----+--------+\n\nTipo de táxi: fhvhv, Total de Registros: 95846120\n\nTotal de registros em todos os arquivos da camada Bronze: 118557687\n"
     ]
    }
   ],
   "source": [
    "# Contagem de registros por partição (ano e mês)\n",
    "taxi_types = ['yellow', 'green', 'fhv', 'fhvhv']\n",
    "total_records = 0\n",
    "\n",
    "for taxi_type in taxi_types:\n",
    "    try:\n",
    "        # Lê a camada Bronze particionada\n",
    "        df_bronze = spark.read.options(**read_options).parquet(f\"{bronze_path}{taxi_type}/\")\n",
    "        \n",
    "        # Agrupa por ano e mês e conta os registros\n",
    "        print(f\"--- Contagem de registros para {taxi_type} por partição ---\")\n",
    "        df_counts = df_bronze.groupBy(\"year\", \"month\").count().orderBy(\"year\", \"month\")\n",
    "        df_counts.show()\n",
    "        \n",
    "        count_total_type = df_counts.agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "        print(f\"Tipo de táxi: {taxi_type}, Total de Registros: {count_total_type}\\n\")\n",
    "        total_records += count_total_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o tipo de táxi {taxi_type}: {e}\\n\")\n",
    "\n",
    "print(f\"Total de registros em todos os arquivos da camada Bronze: {total_records}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Ingestao_Bronze_taxis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
